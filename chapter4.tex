%===================================== CHAP 4 =================================

\chapter{Research Method}\label{chap:4}

The purpose of this project is to create a prototype to improve students' motivation for applying for an exchange program by recommending courses and universities. The research questions were first created after defining the project's purpose and refined after identifying the background, motivation, relevant theory and related work through the preliminary research. The research questions were answered by evaluating a prototype created using the design and creation strategy. The final evaluation was done by using primarily quantitative data analysis on data generated by questionnaires and an offline experiment.

Oates'\cite{oates2005researching} model of the research process was used as a guideline for selecting the most appropriate research methods. The model and the specific methods chosen is displayed in Figure \ref{fig:research_process}. This chapter first presents the chosen research strategy. Next, the data generation methods and their respective data analysis methods are described. Lastly, the practical and ethical issues which influenced and limited the research project are discussed. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/research_process.png}
    \caption[Research process]{Outlined research process for the project}
    \label{fig:research_process}
\end{figure}

\section{Research Strategy: Design and Creation}

The design and creation strategy focuses on creating computer-based artefacts which in some way contributes with new knowledge \cite{oates2005researching}. An artefact can be either be a contribution to new knowledge in itself, a tool used to answer questions, or the end-product of a development process focused project. This strategy adds research properties to regular software development by including academic qualities such as analysis, explanation, argument, justification and critical evaluation.

This project utilized the design and creation strategy to develop a prototype named Utsida. Utsida is an information system (IS) built from the identified system requirements (sec. \ref{sec:requirements}) and is used as a tool to answer both RQ1 and RQ2. The design and creation strategy follows the steps of the Design Science Research (DSR) process model defined by Vaishnavi and Kuechler \cite{vaishnavi2004design}. These steps are: Awareness, Suggestion, Development, Evaluation, and Conclusion.

\subsection{Awareness and Suggestion}
The awareness step involves finding and defining the problem domain, while in the suggestion step, the problem is refined into a tentative idea on how it might be solved. The domain was defined by the preliminary research (Ch. \ref{chap:2}), through the production of research questions (sec. \ref{RQ}) and by gaining knowledge on relevant theory (Ch. \ref{chap:3}). When the problem had been properly identified, an initial suggestion for the system was made. This suggestion included the first design details for the system which initiated the iterative development process.

\subsection{Development}

The development step implements the idea formed in the suggestion step and decides how it should be done and what software development process to use. To answer the research questions it was necessary to produce data on user interaction and satisfaction. It was, therefore, essential that the web application part of Utsida was easy and intuitive to use. To ensure high user satisfaction, the software development of Utsida was done in an iterative process. This included communicating with potential users throughout the development and conducting usability tests. The spiral model, introduced by B. W. Boehm \cite{boehm1988spiral}, is a common model for iterative development, and was chosen as the foundation for the software development process of Utsida. Each spiral in the model includes steps for analyzing risks and requirements, development and testing of a prototype, planning the next iteration, and determining the objective of the next iteration. The model was modified to the needs of the study. Figure \ref{fig:development_process} displays the chosen development process, where the main differences from the spiral model are less risk assessment, and less time spent on validation and planning. The changes were done to be able to quickly implement new functionality, ideas, and get them tested to see if they were feasible. After each development step the prototype was tested on students through usability tests and hallway testing. The next step, Verification and validation, evaluated the prototype according the requirements and goals. The development was then restarted if found needed or terminated if the prototype was deemed ready for the final evaluation.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{fig/research_process_2.png}
    \caption[Development process]{Iterative development process of Utsida}
    \label{fig:development_process}
\end{figure}

\subsubsection{Usability Testing}
The usability tests in each development cycle were performed on the web application to evaluate and ensure sufficient usability. The test participants were selected and consisted in a large degree of students who had an understanding of the current approach for applying for an exchange program. Students from different programs were tested to ensure a broad specter of backgrounds. These tests helped find flaws in both general functionality and design so that the system was as reliable as possible before conducting the final online test. Data was collected by interviewing the testers, observing their use of the system, and have them fill out a System Usability Scale (SUS)-schema \cite{brooke1996sus}. During the interview the interviewee was asked questions such as \enquote{How would you expect this functionality to work?}, \enquote{Was it easy to navigate the system?} and \enquote{Was it intuitive to understand how that functionality works?}. After each test, the data was be analyzed using the SUS score \cite{brooke1996sus}.

\subsection{Evaluation and Conclusion}

The evaluation step includes assessing the developed prototype, while the conclusion step involves analyzing the results and knowledge gained. The final evaluation and conclusion was done by analyzing and discussing the data generated by the chosen data generation methods. Several methods were reviewed. Because there was no client, or persons with high stakes in this project, interviews were not viewed upon as a feasible method to generate data. Data generation through documents were also not viable because of no existing documents about the problem context. Observation were in a sense used for usability tests but were not directly relevant for a final evaluation. However, questionnaires and experiments are standard methods used in the evaluation of recommender systems \cite{oates2005researching}. They were therefore chosen as the designated data generation methods. Questionnaires made it possible to obtain generalized opinions about the system's motivational effect and the offline experiment made it possible to objectively evaluate the suitability of the recommendations without user bias. Two questionnaires were used, questionnaire 1 and 2, and a single offline experiment. Questionnaire 2 was designed as a user study and the offline experiment as an offline evaluation, both known methods to evaluate recommender systems \cite{shani2011evaluating}. The results of these data generations methods are presented in Chapter 6, and discussed in Chapter 7. 

\section{Questionnaires}

Two questionnaires were used to produce the data needed to answer the research questions. The first, questionnaire 1 (sec. \ref{sec:questionnaire_1}), was conducted during the development of the system with the goal of learning what students think are the most important motivational factors for choosing an exchange location. The second, questionnaire 2 (sec. \ref{sec:questionnaire_2}), was conducted after the system prototype was completed, as a final evaluation and user study of Utsida.

Both questionnaires were self-administrated, which promoted the opportunity to collect data from a representative number of students at once. The target group of the questionnaires were students who had some experience with exchange studies. To acquire a large amount of students who were in this group, a cooperation with the OIR was made and they agreed to publish the questionnaires on their Facebook page with approximately 3300 followers, most of them being in the target group of this study.

In order to keep the margin of error to a minimum it was important to get feedback from as many students as possible. According to the Norwegian Centre for Research Data \cite{utvekslingsopphold}, 3192 students at NTNU did an exchange program in 2016. The amount of students at NTNU who are interested in study exchange is hard to estimate, but the total target population for the questionnaires was estimated to be roughly 10 000. According to Krejcie and Morgan \cite{krejcie1970determining} a sample size of approximately 370 is optimal (i.e. yield a margin of error less then 5\%) with a population size of 10000. However, considering the limitations of budget, time and marketing resources, the expected sample size in this study was considerably lower than the optimal size. Hence, a sample size goal of 100 students was set for questionnaires. With a population of 10 000 and confidence level of 95\% this would give a margin of error of approximately 10\% \cite{yamane1973statistics}. 

The majority of the questions in the questionnaires were closed questions formed as either: a Likert Scale \cite{allen2007likert}, a semantic differential scale \cite{osgood1952nature} or with \enquote{Yes}, \enquote{No}, or \enquote{Don't know} options. Also, the students were asked optional open questions where they could articulate their opinions. 

\subsection{Validity and Reliability}

Content validity is concerned with whether the questions in the questionnaire represents a good sample of the actual problem to be investigated \cite{oates2005researching}. This means that the selected questions should cover all the different evaluations needed to answer the research questions accurately. Feedback from potential questionnaire participants and experts were used to evaluate the validity, and the questions were formed with the possible evaluation criterion of the research questions in mind. For RQ1 the criteria is the use of the system and its motivational effect. For RQ2 the evaluation is if the system recommends viable universities and courses.

Construct validity means that the questionnaire is evaluating what it was intended to evaluate, and not other aspects \cite{oates2005researching}. Both questionnaires were designed to ensure the construct validity by using clear and straightforward questions that were in the scope of the research questions. An iterative process was followed in the design of the questionnaires to remove or edit possible irrelevant or poorly formulated questions. 

The measurement of reliability is that the questionnaire yields consistent results. The reliability is difficult to assess in this study as the questionnaires were only published once, and they did not evaluate the system over time. Evaluating the prototype over time with part of the participants could also have distorted the data by giving them more time to be familiar with the prototype. However, one way to measure the reliability or internal consistency is using Cronbach's alpha \cite{bland1997statistics}, which was done on the Likert scale questions in questionnaire 2.

\subsection{Bias}

When using questionnaires as the primary method to collect data, several risks of bias can be introduced, including central tendency bias, acquiescence bias, social desirability bias and non-response bias \cite{furnham1986response}. This section presents these biases, how they could affect the results and how the risk of bias was reduced.  

\paragraph{Central tendency bias} is a risk when the questions have extreme response categories, this is especially relevant for the Likert scale and Semantic Differential Scale. This bias means a participant is more likely answer in the middle of the scale. A lower point scale was used in questionnaire 2 to reduce the central tendency. Both questionnaires also aimed to have clear questions that reduced the likelihood of participants to answer in the middle of the scale.

\paragraph{Acquiescence bias} is when the participant is likely to answer positively on a question \cite{cronbach1946response}. To reduce the acquiescence bias, the questions avoided being strongly positively based. However due to the nature of the questions having less social desirability and not being sensitive an acquiescence bias was less likely to have a large impact. 

\paragraph{Social desirability bias} influence a participant to deny undesirable traits or positively answer questions that are socially desirable. Due the desirability bias questions were also striven to be neutral statements which the participants can state their degree of agreement as given by the Likert scale. 

\paragraph{Non-response bias} is when respondents' answers are considerably different from the possible nonrespondent answers. Considering the expected response rate was a small proportion of the full population, non-response bias was unavoidable. Those who did not participate might think the research sounds uninteresting, and if they participated, they might respond with more negative answers. Oppositely, the target population for the questionnaires were students who are already had an interest in exchange studies, which might influence responses to be more positive. Non-response bias was taken into consideration is the discussion of the results.



\subsection{Questionnaire 1: Motivational Factors}\label{sec:questionnaire_1}

The goal of questionnaire 1 was to gain knowledge on important factors for students when they choose an exchange location and university. The results from this questionnaire was the basis for assigning the weight values of the attributes in exchange experience concept and choosing which attributes to include. 

The main question in the questionnaire was closed and formed according a 7-point semantic differential scale (see fig. \ref{fig:semantic_scale}). A 7-point scale was chosen to better fit the model in myCBR Workbench where the weights are represented in a range from 1-10. The participant ranked each motivational factor according to the particular decisiveness of that factor. Questionnaire 1 also included an open question which asked for input on other important motivational factors. See appendix \ref{app:questionnaire1} for all the questions included in questionnaire 1. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{fig/question1.png}
    \caption{Question format in questionnaire 1}
    \label{fig:semantic_scale}
\end{figure}

\subsubsection{Data analysis}

The numerical data from the questionnaire was analyzed in a quantitative manner, by calculating the mean, standard deviation and coefficient of variation for each attribute. The coefficient of variation was calculated to show the spread relative to the mean of the data and to rank each attribute against each other for the purpose of weighting the attributes in the CBR-RS.

The open question was analyzed in a qualitative manner by using theme analysis \cite{oates2005researching}. First each question was reduced to only include important sections related to the goal of the question. All unrelated parts of text was removed. Each answer was then analyzed by finding related themes of answers and mapping them in a table. The number of occurrences of each theme was then counted and ranked by importance.

\subsection{Questionnaire 2: User Testing of Utsida}\label{sec:questionnaire_2}

The three goals of questionnaire 2 was to 1. Evaluate and analyze how Utsida affects students motivation for applying for an exchange program, 2. Find out whether students who have already been on an exchange program thinks Utsida would make their process easier, and 3. Evaluate if Utsida gave relevant recommendations to the user.

The questionnaire consisted of two main parts of questions; one for reviewing the motivation and use of the application, and one for evaluation of CBR-RS' recommendations. All the questions from both parts can viewed in Appendix \ref{app:questionnaire2_questions}.

\subsubsection{Part 1: Motivational Effect and Use}
The questions in the first part were on a Likert-type form \cite{likert1932technique}. This question form is designed as a five-point scale where each option represents a degree of agreement to a given statement. This way, the questions can be neutral statements, where the recipients of the questionnaire could select their the level of agreement they relate to the most, leaving as little room for bias as possible. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/q2_question_form.PNG}
    \caption[Question format for selecting level of agreement on a statement]{Question format for selecting level of agreement on a statement in questionnaire 2}
    \label{fig:q2_question_form}
\end{figure}

The questions in the first part of questionnaire 2 was targeted to find out what effect Utsida may have on the students motivation to apply for an exchange program (RQ1). The students were presented with slightly different questions depending on whether they had participated in an exchange program or not.

\subsubsection{Part 2: Evaluation of Recommendations}

The second part of the questionnaire targeted RQ2 by letting students evaluate the recommendations they received from Utsida, and evaluate the results from two pre-defined queries to the CBR-RS. The question form was designed to have simple \enquote{Yes}, \enquote{No}, \enquote{Don't know} answers to easier determine the amount of students that received relevant recommendations. The pre-defined query recommendations also included a question which asked the students to rate the recommendations (see Figure \ref{fig:q2_rate_search}). The pre-defined queries (App. \ref{pre_defined_search_table}), were included to evaluate the recommendations for a common specific search and to avoid the evaluations being affected by participants' personal attribute preferences.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/rate_search.png}
    \caption[Question format for evaluating suitability of recommendations]{Question format for evaluating suitability of recommendations for a pre-defined query in questionnaire 2}
    \label{fig:q2_rate_search}
\end{figure}


\subsubsection{Data analysis}
The data analysis on the results from questionnaire 2 was done similar to questionnaire 1 by using quantitative analysis on the closed questions and simple qualitative analysis on the open-ended question. The closed questions on a Likert scale form produce ordinal data, meaning that the standard range between the values on the scale can not be determined. The data was, therefore, analyzed by the use of mode and frequencies. For the \enquote{Yes}, \enquote{No} or \enquote{Don't know} questions however, only percentages and number of answers for each option is presented. Cronbach's alpha was used to asses the internal consistency of the Likert scale questions regarding motivation. 

\section{Offline Experiment on the CBR-RS}\label{sec:observation_test}

An offline experiment method introduced by Ricci et al. \cite{ricci2011introduction} was used to evaluate the relevancy of the recommendations of the CBR-RS, and thus contributed to answering RQ2. This is a specific method to evaluate recommendation systems in an offline setting without user influence, and should not be misinterpreted as the \textit{Experiment} research strategy.

The offline experiment was conducted with 20 queries that were generated to simulate 20 unique users (App. \ref{app:user_queries}). Each query was sent to two different concept configurations. One with adjusted similarity measures, taxonomies and weights, and one simulating standard exact match search. The simulation was done by only using exact match similarity measures and equal attribute weights. The scores for each outputting recommendation was then given according to the score metric in Table \ref{tab:offline_test} and the total score for each configuration was the mean score of each query. The hypothesis was that the configuration with adjusted similarity measures and weights would score higher than the configuration with a simple full match search. 

This method made it possible to test the CBR-RS with a large set of inputs at a low cost. One downside of the method is that the queries only represent a small part of the possible user searches. This was mitigated by selecting a wide range of possible attributes and users profiles. Another disadvantage was that the offline experiment did not evaluate the CBR-RS with actual users, but by having continuous usability tests and user test with questionnaire 2, this was a non-issue. The courses are not included in this experiment because it is difficult to objectively measure and score their relevancy.


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{D0E0E3}}l |l|l|l|}
\hline
\multicolumn{4}{|c|}{\cellcolor[HTML]{A4C2F4}{\color[HTML]{333333} Rating scale for recommendations (0-10 points)}}                                                                                             \\ \hline
Attribute           & \cellcolor[HTML]{D0E0E3}0 points                                         & \cellcolor[HTML]{D0E0E3}1 point                                             & \cellcolor[HTML]{D0E0E3}2 points \\ \hline
Institute           & \begin{tabular}[c]{@{}l@{}}Wrong faculty \\ wrong institute\end{tabular} & \begin{tabular}[c]{@{}l@{}}Correct faculty\\ wrong institute\end{tabular}   & Correct Institute                \\ \hline
Year                & Before 2011                                                              & 2011-2013                                                                   & 2014-2017                        \\ \hline
Geographic Location & \begin{tabular}[c]{@{}l@{}}Wrong country \\ wrong continent\end{tabular} & \begin{tabular}[c]{@{}l@{}}Correct continent\\ wrong country\end{tabular}   & Correct country                  \\ \hline
University          & \multicolumn{2}{c|}{No match}                                                                                                                          & Perfect match                    \\ \hline
Language            & No match                                                                 & \begin{tabular}[c]{@{}l@{}}The language is \\ part of the list\end{tabular} & Perfect match                    \\ \hline
Ratings             & \multicolumn{3}{c|}{0.5 points for each rating in range (-1, rating +1)}                                                                                                                  \\ \hline
\end{tabular}%
}
\caption[Score matrix for offline experiment]{Score matrix used to evaluate the recommendations in the offline experiment}
\label{tab:offline_test}
\end{table}

\subsection*{Data Analysis}
The offline experiment generated quantitative data that was statistically analyzed. The central tendency were analyzed with the mean and standard deviation measures, and a paired t-test was conducted to define the significance of the result. The confidence level was 95\% with a p-value limit of 0.05.

\section{Ethical Issues}
Some ethical issues regarding data collection and storage were considered in this study. The designated data collection methods kept all participants anonymous, but some meta-data was collected, such as which faculty and university they belong to. Furthermore, as an incentive for students to answer the questionnaires, lottery prizes were promoted for participating. To be able to enter the lottery, the participants had to enter their e-mail. This step was entirely optional. When each questionnaire was closed, the answers were stored in a Google Spreadsheet anonymously, and the entered e-mails were scrambled in a random order so that they were not linked to their answers. This way, there was no way to identify each answer. After the lottery was completed all the emails were deleted and only the winner contacted. 

Informed consent was maintained in the questionnaires by informing the participants about the purpose and goals for the questionnaires, and by stating that the entire process for all participants is entirely voluntarily and that they can stop any time they want.

During the online test of the system, the users created an account which included storing their chosen password, e-mail, name and institute at NTNU. This information could not be linked to the questionnaire they took, but it still raised an ethical issue, considering the authors had access to this data. During the development of the system, steps were taken to ensure the confidentiality of the users of the system by using encryption and proper security measures.


\section{Practical Issues}
This study was performed by two students, with one supervisor to guide the project. There was a minuscule amount of monetary funds, and the available project duration was limited to nine months. The data which served as the results was primarily produced by real users. With the minimal budget, a practical issue was to produce incentive for enough students to answer the surveys. Furthermore, because of the limited time, both larger administrated tests and self-administrated tests were considered out of reach.

\cleardoublepage




